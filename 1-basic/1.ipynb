{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable not set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3367cf",
   "metadata": {},
   "source": [
    "### Example 1: Simple LLM Call with streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "## agr koi b messsage used karne ke liye y used karan pare ga \n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3106c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=init_chat_model(\"groq:llama-3.1-8b-instant\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create messages\n",
    "message = [ \n",
    "    SystemMessage(\"you are a helpfull AI assistant \"),\n",
    "    HumanMessage(\"What are the top 2 besnifts of langchain \")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##invoke the model\n",
    "response=model.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(message):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    ## yh aik alternative method he print karwane k liye "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15caaf82",
   "metadata": {},
   "source": [
    "### Dynamic Prompt Template \n",
    "## aik addition prompt de skty hen jo instruction llm ko dena he usko prompt template me dal dety hen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "## create translation app \n",
    "\n",
    "translation_template=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You  are a professional translator.Translte the follow text {text} from {source_language} to {target_language}\"),\n",
    "    (\"user\",\"{text}\")\n",
    "])\n",
    "\n",
    "## using the tempalte \n",
    "prompt=translation_template.invoke({\n",
    "    \"source_language\": \"English\",\n",
    "    \"target_language\": \"Russia\",\n",
    "    \"text\":\"LangChain is best for Z generation\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bacc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_response=model.invoke(prompt)\n",
    "print(translated_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399c228",
   "metadata": {},
   "source": [
    "### Building my First Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c386e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "def create_story_chain():\n",
    "    ##Template for story generation \n",
    "    story_prompt=ChatPromptTemplate.from_messages (\n",
    "        [\n",
    "            (\"system\",\"You are a creative story teller.Write a short and engaging story based on given theme\"),\n",
    "            (\"user\",\"Theme:{theme}\\n Setting :{setting}\")\n",
    "        ]\n",
    "        ## Yh prompt aik model ko jaaiey ga \n",
    "    )\n",
    "\n",
    "    ## Template for story analysis \n",
    "    analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\"You are literaly best.Analyze th following story and provide insights.\"),\n",
    "            (\"user\",\"{story}\")\n",
    "        ]\n",
    "    )\n",
    "    story_chain=(\n",
    "        story_prompt| model | StrOutputParser()\n",
    "        ## pehly story prompt bny ga vh jaoey ga model me or phir stroutputparser me \n",
    "        ## stroutparser direcct content le leta he LLM se or vh show karta he\n",
    "    )\n",
    "\n",
    "    def analyze_story(story_text):\n",
    "        return {\"story\": story_text}\n",
    "    \n",
    "    analysis_chain = (\n",
    "        story_chain\n",
    "        | RunnableLambda(analyze_story)\n",
    "        | analysis_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return analysis_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=create_story_chain()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448559fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.input_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Corrected spelling + formatted code\n",
    "result = chain.invoke({\n",
    "    \"theme\": \"artificial intelligence\",\n",
    "    \"setting\": \"a futuristic city\"\n",
    "})\n",
    "\n",
    "print(\"Story and Analysis:\")\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f791c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
